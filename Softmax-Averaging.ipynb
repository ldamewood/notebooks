{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, I explain how find the ensemble average of classification models using the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble average of logistic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression, the predicted outcome $y(w,x)$ for an observed set of features $x$ is\n",
    "\\begin{align}\n",
    "y(w,x)\n",
    " &= w_0 + w_1 x_1 + \\cdots + w_n x_n\n",
    "\\end{align}\n",
    "where $w_{i}$ is the learned weights of the linear model. In the case of logistic regression used for binary outcomes, the predicted outcome is limited to be between $0$ and $1$. Typically, the sigmoid function $\\sigma(z)$ is used\n",
    "\\begin{align}\n",
    "y(w,x)\n",
    " &= \\sigma(z) \\\\\n",
    " &= \\frac{1}{1+e^{-z(w,x)}} \\\\\n",
    "z(w,x)\n",
    " &= w_0 + w_1 x_1 + \\cdots + w_n x_n.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple models that use the logistic function as the outcome, the ensemble average can be calculated by first taking the mean value of the function $z$ and then inputing the result into the logistic function\n",
    "\\begin{align}\n",
    "\\bar z(w,x)\n",
    " &= \\frac{1}{N}\\sum_i z_i \\\\\n",
    " &= \\frac{1}{N}\\left[\\sum_{i} w_{i0} + w_{i1} x_1 + \\cdots + w_{in} x_n\\right] \\\\\n",
    "\\bar y(w,x)\n",
    " &= \\sigma(\\bar z)\n",
    "\\end{align}\n",
    "where $N$ is the number of models in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning codes only give you the predicted outcome of each model. When this is the case, the logistic function can be inverted to obtain the $z_i$s. The inverse logistic function is the logit function\n",
    "\\begin{align}\n",
    "\\mathrm{logit}(y)\n",
    " &= \\ln\\frac{y}{1 - y}\n",
    "\\end{align}\n",
    "so the ensemble mean is\n",
    "\\begin{align}\n",
    "\\bar y = \\sigma\\left(\\frac{1}{N}\\sum_i \\mathrm{logit}(y_i) \\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation for softmax ensemble average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function is a generalization of the sigmiod function for when there are multiple outcomes. It is defined as\n",
    "\\begin{align}\n",
    "y(z_i) &= \\frac{e^{z_i}}{\\sum_{k}e^{z_k}}\n",
    "\\end{align}\n",
    "and it reduced to the sigmoid function when there are only two outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverting the softmax function in order to take the mean follows the same steps as inverting the sigmoid function. The only difference is now the denominator is more complicated. When there are multiple models to choose from, I will define a common softmax function for all of them\n",
    "\\begin{align}\n",
    "y(z_{ij}) &= \\frac{e^{z_{ij}}}{\\sum_{k}e^{z_{kj}}}\n",
    "\\end{align}\n",
    "where the index $j$ identifies which model is used. The denominator is a constant for each model, so I will replace it with $f_j$. Additionally, I will take the log of the equation and solve for $z_{ij}$\n",
    "\\begin{align}\n",
    "y_{ij}\n",
    " &= e^{z_{ij}}f_j \\\\\n",
    "\\ln y_{ij}\n",
    " &= z_{ij} + \\ln f_j \\\\\n",
    "z_{ij} &= \\ln y_{ij} - \\ln f_j.\n",
    "\\end{align}\n",
    "These are the terms I want the average over all models.\n",
    "\\begin{align}\n",
    "\\bar z_i\n",
    " &= \\frac{1}{N}\\sum_j z_{ij} \\\\\n",
    " &= \\frac{1}{N}\\sum_j \\ln y_{ij} - \\frac{1}{N}\\sum_j \\ln f_j.\n",
    "\\end{align}\n",
    "Finally, the softmax of the average $z_i$'s is\n",
    "\\begin{align}\n",
    "softmax(\\bar z_i)\n",
    "&= \\frac{e^{\\frac{1}{N}\\sum_j \\ln y_{ij} - \\frac{1}{N}\\sum_j \\ln f_j}}\n",
    "{\\sum_k e^{\\frac{1}{N}\\sum_j \\ln y_{kj} - \\frac{1}{N}\\sum_j \\ln f_j}} \\\\\n",
    "&= \\frac{e^{\\frac{1}{N}\\sum_j \\ln y_{ij}}}\n",
    "{\\sum_k e^{\\frac{1}{N}\\sum_j \\ln y_{kj}}} \\\\\n",
    "&= \\frac{e^{\\overline{\\ln y_{ij}}}}\n",
    "{\\sum_k e^{\\overline{\\ln y_{kj}}}} \\\\\n",
    "\\end{align}\n",
    "Some examples are next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "First, I define a simple softmax function using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.0478e-03   5.5390e-05   9.9890e-01]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision = 4)\n",
    "\n",
    "def softmax(z):\n",
    "    return (np.exp(z).T / np.exp(z).T.sum(axis = 0)).T\n",
    "\n",
    "# Higher values produce higher probability.\n",
    "z = [3.14,0.2,10.]\n",
    "print(softmax(z))\n",
    "\n",
    "# All probabilities add up to 1\n",
    "assert np.isclose(softmax(z).sum(), 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a typicaly classification model with multiple outcomes, the output may look like a matrix where the number of rows is the number of models and the columns represent the different outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2756  0.1532  0.1844  0.1562  0.2306]\n",
      " [ 0.1298  0.2145  0.1948  0.2297  0.2312]\n",
      " [ 0.1725  0.1606  0.2515  0.2531  0.1624]]\n"
     ]
    }
   ],
   "source": [
    "# 3 models with 5 outcomes:\n",
    "y = softmax(np.random.random((3,5)))\n",
    "print(y)\n",
    "\n",
    "# All probabilities add up to 1\n",
    "assert np.allclose(y.sum(axis=1), [1.,1.,1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, it's really simple to take the ensemble average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1872  0.1777  0.2126  0.2129  0.2096]\n",
      "[ 0.1926  0.1761  0.2102  0.213   0.2081]\n"
     ]
    }
   ],
   "source": [
    "y_bar = softmax(np.log(y).mean(axis=0))\n",
    "print(y_bar)\n",
    "\n",
    "assert np.isclose(y_bar.sum(), 1.)\n",
    "\n",
    "# It is not the same as taking the mean directly.\n",
    "print(y.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why, intuitivly, the ensemble mean is better than just taking the mean? I will try to make one model disagree heavily with the others. Model number zero gives the opposite results as the other two models, which agree perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.0000e-10   1.0000e+00   1.0000e+00   1.0000e+00   1.0000e+00]\n",
      " [  1.0000e+00   1.0000e-10   1.0000e-10   1.0000e-10   1.0000e-10]\n",
      " [  1.0000e+00   1.0000e-10   1.0000e-10   1.0000e-10   1.0000e-10]]\n"
     ]
    }
   ],
   "source": [
    "y[0,0] = 0.\n",
    "y[0,1:] = 1.\n",
    "y[1:,0] = 1.\n",
    "y[1:,1:] = 0.\n",
    "# Can't let the probability be zero because of the log\n",
    "y.clip(min = 1.e-10, out = y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the outcomes using the two types of averaging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.9815e-01   4.6330e-04   4.6330e-04   4.6330e-04   4.6330e-04]\n",
      "[ 0.6667  0.3333  0.3333  0.3333  0.3333]\n"
     ]
    }
   ],
   "source": [
    "print(softmax(np.log(y).mean(axis=0)))\n",
    "print(y.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this method appears to ignore outliers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
